# 如何从现成的视频中生成一个虚拟人物

## 所用到的神经网络框架模型

* [SimSwap](https://github.com/neuralchen/SimSwap) 用于人脸替换
* [Wav2Lip](https://github.com/Rudrabha/Wav2Lip) 用于语音替换
* [KAIR](https://github.com/cszn/KAIR) 人脸增强（提高清晰度）
* 获取语音（可以使用讯飞等支持语音合成的供应商）



# 站在隐私保护的角度，请不要以未授权的视频做实验。需要虚拟人脸的可以使用[styleGAN](https://github.com/NVlabs/stylegan2) 批量生成!!!



## Lets go!

***Note***: 多种神经网络python版本、torch或者tensorflow版本可能不一样，推荐anaconda或者miniconda创建虚拟环境。另外， *SimSwap* 以及 *Wav2Lip* 都可以使用CPU运行，但是 *KAIR* 需要GPU环境（似乎超分相关的神经网络都相当耗资源）

1. SimSwap 环境搭建

   * 下载源代码

   * 使用 conda 创建新的虚拟环境

     ```
     conda create -n simswap python=3.8
     ```

     官方推荐是 3.6 版本，不过3.8版本也可以。然后安装 torch相关，推荐是使用 conda安装（pip安装其实也可以,如果只使用CPU运行的话其实是没有区别的）

     ```
     conda install pytorch==1.8.0 torchvision==0.9.0 torchaudio==0.8.0 cudatoolkit=10.2 -c pytorch
     ```

     我这里安装的是torch 1.10.2版本

     ```
     pip install insightface==0.2.1 onnxruntime moviepy
     ```

   * 下载与训练模型，有[谷歌云盘](https://drive.google.com/drive/folders/1jV6_0FIMPC53FZ2HzZNJZGMe55bbu17R?usp=sharing)和[百度云盘](https://pan.baidu.com/s/1wFV11RVZMHqd-ky4YpLdcA)（jd2v）, 包括两个文件 **checkpoints.zip** 和 **arcface_checkpoint.tar**，把 **arcface_checkpoint.tar** 保存到 **./arcface_model** 文件夹下，把**checkpoints.zip** 解压之后放到根目录

   * 运行以下指令

     ```
     python test_video_swapsingle.py --crop_size 224 --use_mask --name people --Arc_path arcface_model/arcface_checkpoint.tar --pic_a_path ./demo_file/0.png --video_path ./demo_file/Asianwoman.mp4 --output_path ./output/multi_test_swapsingle.mp4 --temp_path ./temp_results --no_simswaplogo
     ```

     主要需要指定以下几个参数：

     * **--pic_a_path**  ：指定样板人物
     * **--video_path** ：指定要替换人脸的视频
     * **--output_path** ：输出路径
     * **--temp_path** ： 保存每一帧的路径
     * **--no_simswaplogo** ： 去除logo,否则会有水印

   * 这个模型的优势在于可以使用一张图片替换视频中人脸，相对于 [faceSwap](https://github.com/deepfakes/faceswap) 需要视频对视频，多图训练的做法更加简单。同时不怎么吃GPU资源，支持指定人物替换。但是多人替换的时候问题比较多，跑他们自己的多人替换demo效果也比较差，同时人物头部动作比较大也会影响最后的替换效果。

   * 存在找不到人脸的情况。请[参考](https://github.com/neuralchen/SimSwap/issues/206), 这是我修改后的[代码](./codes/simswap/test_video_swapsingle.py)

2. Wav2Lip 环境搭建

   * 下载源代码

   * 使用 conda 创建新的虚拟环境

     ```
     conda create -n simswap python=3.6
     ```

   * 安装ffmpeg ([windows](https://blog.csdn.net/chy466071353/article/details/54949221) , [linux](https://www.cnblogs.com/wangsongbai/p/12293638.html) ,[mac](https://www.cnblogs.com/renhui/p/8458150.html)未验证，因为我没有mac)

   * pip 安装依赖 

     ```
     pip install -r requirements.txt
     ```

     下载速度慢可以 ```-i https://pypi.douban.com/simple/```

   * 下载预训练模型，我用的是[这个](https://iiitaphyd-my.sharepoint.com/:u:/g/personal/radrabha_m_research_iiit_ac_in/Eb3LEzbfuKlJiR600lQWRxgBIY27JZg80f7V9jtMfbNDaQ?e=TBFBVW) 

   * 测试

     ```
     python inference.py --checkpoint_path <ckpt> --face <video.mp4> --audio <an-audio-source> 
     ```

     主要指定以下几个参数：

     * **--checkpoint_path** ： 预训练模型路径
     * **--face** ： 需要替换嘴型的视频
     * **--audio** ： 音频文件

     ***note***: 如果音频文件比视频文件长，可能存在重复播放视频的问题。

3.  完成以上内容之后，需要使用 ffmpeg 将视频转为图像帧

   ```
   ffmpeg -i input.mp4 -r 1 -f image2 image-%3d.jpeg
   -i : 指定输入文件
   -r : 帧数 1 (每一秒抽取一张图片，一般来说视频应该是一秒钟24帧)
   -f : 指定格式化的格式为image2
   生成的结果 image-%3d.jpeg  %3d是指3位数字
   ```

4. KAIR 环境搭建

   KAIR搭建稍微复杂一些，因为这个框架包含了很多模型，包括视频、图像超分算法，而我们要用的是人脸增强（用于提高人脸清晰度）模型。

   * 安装下载不再赘述，都大同小异，不过这个需要GPU资源，请确保自己电脑GPU、cuda、cudnn安装
   * 安装好依赖以后，需要下载预训练模型。KAIR人脸增强用到的预训练模型较多，包括 [RetinaFace-R50](https://public-vigen-video.oss-cn-shanghai.aliyuncs.com/robin/models/RetinaFace-R50.pth?OSSAccessKeyId=LTAI4G6bfnyW4TA4wFUXTYBe&Expires=1961116085&Signature=GlUNW6%2B8FxvxWmE9jKIZYOOciKQ%3D), [GPEN-BFR-512](https://public-vigen-video.oss-cn-shanghai.aliyuncs.com/robin/models/GPEN-BFR-512.pth?OSSAccessKeyId=LTAI4G6bfnyW4TA4wFUXTYBe&Expires=1961116208&Signature=hBgvVvKVSNGeXqT8glG%2Bd2t2OKc%3D)(我用的是这个，可以在[这里](https://github.com/yangxy/GPEN) 选择其他**GPEN** 模型下载，**RetinaFace**是必要的)。
   * 下载完成之后把以上两个模型放入 **model_zoo** 文件夹下
   * 修改 **main_test_face_enhancement.py** main函数中测试图片路径即可进行人脸增强

5. 完成上一步操作之后，再将图像转视频，音视频融合

   ```
   ffmpeg -i image-%3d.jpeg out.mp4
   -i : 指定图片内容
   最终结果输出为out.mp4
   ```

   ```
   ffmpeg  -i  video.mp4  -i  audio.mp3  -vcodec  copy  -acodec  copy  音视频.mp4
   ```

   

## 到此结束



## 后续

1. 搞个神经网络生成音频 text-to-speech

